{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取資料\n",
    "data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# 將標題和簡短描述結合成一個新的欄位\n",
    "data['text'] = data['headline'] + ' ' + data['short_description']\n",
    "test_data['text'] = test_data['headline'] + ' ' + test_data['short_description']\n",
    "# 檢視前五筆資料\n",
    "# print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>      1982\n",
      "<class 'float'>      18\n",
      "Name: text, dtype: int64\n",
      "[ 867 1616 1617 1618 1619 1631 1656 1657 1658 1659 1662 1670 1671 1677\n",
      " 1678 1681 1682 1683]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#有些資料沒有short_description\n",
    "print(data['text'].apply(type).value_counts())\n",
    "# 找到值為浮點數的索引\n",
    "nan_indices = np.where(data['text'].apply(type) == float)[0]\n",
    "print(nan_indices)\n",
    "\n",
    "#將 data['text'] 中的所有非字串類型都轉換為空字串\n",
    "data['text'] = data['text'].fillna('')    #空值的位置填充為空字符串\n",
    "test_data['text'] = test_data['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jets', 'chairman', 'christopher', 'johnson', 'wo', \"n't\", 'fine', 'players', 'anthem', 'protests', '“', 'never', 'want', 'put', 'restrictions', 'speech', 'players', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "# 使用NLTK將文本轉換為整數列表\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "# nltk.download('punkt') #NLTK中提供的句子分割器和單詞分割器\n",
    "# nltk.download('stopwords') #下載停用詞\n",
    "\n",
    "# 獲取英文停用詞集合\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 將文本轉換為小寫並去除停用詞\n",
    "def preprocess_text(text):\n",
    "    # 分詞\n",
    "    tokens = word_tokenize(text)\n",
    "    # 轉換為小寫並去除停用詞\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# 對數據進行處理\n",
    "data['text_tokenized'] = data['text'].apply(preprocess_text)\n",
    "test_data['text_tokenized'] = test_data['text'].apply(preprocess_text)\n",
    "\n",
    "print(data.iloc[0]['text_tokenized'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# 定義一個函數，將文本列表轉換為整數列表\n",
    "def yield_tokens(data_list):\n",
    "    for tokens in data_list:\n",
    "        yield tokens\n",
    "\n",
    "# 創建一個詞彙表(build_vocab_from_iterator預期接收一個可迭代的tokens序列)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data['text_tokenized']), specials=[\"<unk>\", \"<pad>\"]) \n",
    "# vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "#將單詞轉換為整數\n",
    "def text_transform(tokenized_text):\n",
    "    return torch.tensor([vocab[token] for token in tokenized_text])\n",
    "\n",
    "# Pad sequences to maximum length and stack\n",
    "padded_text = pad_sequence(list(map(text_transform, data['text_tokenized'].values)), batch_first=True)\n",
    "data_set = torch.utils.data.TensorDataset(padded_text, torch.tensor(data['category'].values))\n",
    "    \n",
    "# 將資料集分割為訓練集和驗證集\n",
    "train_size = int(0.8 * len(data_set))\n",
    "val_size = len(data_set) - train_size\n",
    "train_dataset, val_dataset = random_split(data_set, [train_size, val_size])\n",
    "\n",
    "# 創建訓練集和驗證集的 DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將測試數據文本轉換為整數序列\n",
    "test_data['text_transformed'] = test_data['text_tokenized'].apply(text_transform)\n",
    "\n",
    "# 將整數序列的文本進行填充\n",
    "padded_test_text = pad_sequence(list(test_data['text_transformed'].values), batch_first=True)\n",
    "\n",
    "# 創建測試集的 DataLoader\n",
    "test_labels = torch.zeros(padded_test_text.shape[0], dtype=torch.int64)\n",
    "test_dataset = torch.utils.data.TensorDataset(padded_test_text, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do you choose the tokenizer for this task? Could we use the white space to tokenize the text? What about using the complicated tokenizer instead? (5%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK（Natural Language Toolkit）是一個廣泛使用的Python自然語言處理庫，具有豐富的功能和工具。選擇NLTK作為分詞器的原因是因為它是一個成熟的自然語言處理庫，它提供了各種文本處理任務所需的功能，包括分詞、詞性標註、句法分析等。\n",
    "是的，我們可以使用空格來對文本進行分詞。空格分詞是一種簡單且直接的分詞方法，特別適用於某些語言和特定的文本數據集。\n",
    "使用複雜的分詞器相對於簡單的空格分詞器來說，可以提供更高級的分詞功能和更準確的分詞結果。使用複雜的分詞器可能有以下優點：處理複雜文本結構、支持多語言處理、提供上下文信息等。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why we need the special tokens like ⟨pad⟩, ⟨unk⟩? (2%)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding Token (⟨pad⟩): 用於將序列填充到固定長度，確保輸入大小一致，以便在神經網絡中進行高效的處理。\n",
    "Unknown Token (⟨unk⟩)： 當遇到未知詞時，它會被替換為 ⟨unk⟩ 標記，從而使模型能夠處理未見過的詞並更好地泛化到新的或未見過的數據。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Briefly explain how your procedure is run to handle the text data. (3%) (e.g. Which\n",
    "tokenizer do you choose? Why? What is your min count setting? etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先將headline以及short_description的資料合併，接著利用NLTK作為tokenizer將句子切成token的型態，接著在建立詞彙表，將token轉換為整數。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Transformer 模型\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_heads, hidden_size, num_layers, num_classes, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positional_encoding = PositionalEncoding(embedding_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_size,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(embedding_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)  # 加上 softmax 函數\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len)\n",
    "        src = self.embedding(src) # shape: (batch_size, seq_len, embedding_size)\n",
    "        src = self.positional_encoding(src) # 添加 positional encoding\n",
    "        src = src.transpose(0, 1) # shape: (seq_len, batch_size, embedding_size)\n",
    "        output = self.transformer(src, src) # shape: (seq_len, batch_size, embedding_size)\n",
    "        output = output.transpose(0, 1) # shape: (batch_size, seq_len, embedding_size)\n",
    "        output = self.fc(output.mean(dim=1)) # shape: (batch_size, num_classes)\n",
    "        output = self.softmax(output)  # 加上 softmax 函數\n",
    "        return output\n",
    "\n",
    "# 模型參數\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 200\n",
    "num_heads = 2\n",
    "hidden_size = 200\n",
    "num_layers = 2\n",
    "num_classes = len(set(data['category'].values))\n",
    "dropout = 0.2\n",
    "\n",
    "# 模型、損失函數、優化器\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Positional Encoding 模塊\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(1)\n",
    "        pe = self.pe[:, :seq_len, :]\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = TransformerModel(vocab_size, embedding_size, num_heads, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 0.01 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95) #每經過1epoch，lr*0.95\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# 下載並加載GloVe預訓練的詞向量模型\n",
    "glove = GloVe(name='6B', dim=embedding_size)\n",
    "\n",
    "# 創建一個與詞彙表大小和嵌入維度相匹配的嵌入矩陣\n",
    "embedding_matrix = torch.zeros(vocab_size, embedding_size)\n",
    "\n",
    "# 將GloVe詞向量加載到嵌入矩陣中\n",
    "for i, token in enumerate(vocab.get_itos()):\n",
    "    if token in glove.stoi:\n",
    "        embedding_matrix[i] = glove.vectors[glove.stoi[token]]\n",
    "\n",
    "# 將嵌入矩陣加載到模型的嵌入層中\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "\n",
    "# 將嵌入層的requires_grad屬性設置為False，以固定詞向量\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.2018, Train Acc: 0.5831, Val Loss: 0.9577, Val Acc: 0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Train Loss: 0.9216, Train Acc: 0.8250, Val Loss: 0.9172, Val Acc: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Train Loss: 0.8895, Train Acc: 0.8569, Val Loss: 0.9281, Val Acc: 0.8125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Train Loss: 0.8535, Train Acc: 0.8969, Val Loss: 0.9005, Val Acc: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Train Loss: 0.8427, Train Acc: 0.9044, Val Loss: 0.9205, Val Acc: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Train Loss: 0.8351, Train Acc: 0.9119, Val Loss: 0.9079, Val Acc: 0.8300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Train Loss: 0.8221, Train Acc: 0.9231, Val Loss: 0.9076, Val Acc: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Train Loss: 0.8130, Train Acc: 0.9325, Val Loss: 0.8955, Val Acc: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Train Loss: 0.8069, Train Acc: 0.9387, Val Loss: 0.9025, Val Acc: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Train Loss: 0.8031, Train Acc: 0.9406, Val Loss: 0.9035, Val Acc: 0.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Train Loss: 0.8039, Train Acc: 0.9406, Val Loss: 0.8965, Val Acc: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Train Loss: 0.8055, Train Acc: 0.9400, Val Loss: 0.9014, Val Acc: 0.8425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Train Loss: 0.8018, Train Acc: 0.9437, Val Loss: 0.9025, Val Acc: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Train Loss: 0.7997, Train Acc: 0.9437, Val Loss: 0.9034, Val Acc: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Train Loss: 0.7987, Train Acc: 0.9469, Val Loss: 0.9057, Val Acc: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Train Loss: 0.7972, Train Acc: 0.9463, Val Loss: 0.9031, Val Acc: 0.8400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Train Loss: 0.7975, Train Acc: 0.9463, Val Loss: 0.9025, Val Acc: 0.8375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Train Loss: 0.7976, Train Acc: 0.9456, Val Loss: 0.9083, Val Acc: 0.8275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Train Loss: 0.7950, Train Acc: 0.9494, Val Loss: 0.9080, Val Acc: 0.8350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Train Loss: 0.7927, Train Acc: 0.9513, Val Loss: 0.9010, Val Acc: 0.8425\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')  # 初始設定為無窮大\n",
    "best_model_weights = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(device), (labels-1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({\"Train Loss\": running_loss / total_train, \"Train Acc\": correct_train / total_train})\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_acc = correct_train / total_train\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # 驗證\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs, labels = inputs.to(device), (labels-1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, labels)\n",
    "            running_val_loss += val_loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_val_loss / len(val_dataset)\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "          \n",
    "    # 保存驗證損失最低的模型參數\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_weights = model.state_dict()\n",
    "\n",
    "# 儲存最佳模型参数\n",
    "torch.save(best_model_weights, \"model.pth\")\n",
    "# torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "model = TransformerModel(vocab_size, embedding_size, num_heads, hidden_size, num_layers, num_classes, dropout).to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def test(dataloader, model, device):\n",
    "    model.eval()                               \n",
    "    y_hats = []\n",
    "    for x, _ in dataloader:                         \n",
    "        x = x.to(device)                       \n",
    "        with torch.no_grad():                   \n",
    "            y_hat = model(x)                     \n",
    "            y_hats.append(y_hat.detach().cpu())\n",
    "    y_hats = torch.cat(y_hats, dim=0)\n",
    "    _, predicted = torch.max(y_hats, 1)  \n",
    "    predicted = (predicted+1).numpy()     \n",
    "    return predicted\n",
    "\n",
    "def save_y_hat(y_hats, file):\n",
    "    \"\"\" Save predictions to specified file \"\"\"\n",
    "    print(\"Saving results to {}\".format(file))\n",
    "    with open(\"submission.csv\", mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"id\", \"category\"])\n",
    "        for i, y in enumerate(y_hats, start=1):\n",
    "            writer.writerow([i, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to submission.csv\n"
     ]
    }
   ],
   "source": [
    "y_hats = test(test_dataloader, model, device) \n",
    "save_y_hat(y_hats, \"submission.csv\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
